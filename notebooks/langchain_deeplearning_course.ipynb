{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning AI cousre: Langchain\n",
    "\n",
    "In this notbook a course of Deeplearning AI will be covered about langchain. The course is available on [Deeplearning](https://learn.deeplearning.ai/langchain/lesson/1/introduction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Video 1: Introduction\n",
    "\n",
    "### Summary\n",
    "\n",
    "#### LANCHAIN: MODELS, PROMPTS & PARSERS\n",
    "You in lesson one, we'll be covering models prompts and parsers. Models refers to the language models. Parsers involves taking the output of these models and parsing it into a more structured format. Lanchain gives an easy set of abstractions to do this type of operation.\n",
    "\n",
    "#### LANCHAIN: CHAT GPT MODEL PROMPTS AND PAUSES\n",
    "Using Lanchain's abstraction for the chat GPT API endpoint. Create a prompt template. Translate the text that is delimited by triple bactics into style that is style. You can then prompt the large language model to get a response.\n",
    "\n",
    "#### LANCHAIN: THE PROMPT TEMPLATES\n",
    "As you build sophisticated applications, prompts can be quite long and detailed. Prompt templates are a useful abstraction to help you reuse good prompts when you can. Using some of Langchain's built in prompts, you can quickly get an application working.\n",
    "\n",
    "#### LANCHAIN: OUTPUT PARSER\n",
    "You can have an LM output JSON and use langchain to parse that output. The running example will be to extract information from a product review and format that output in adjacent format. Similar to our earlier usage of a prompt template, let's create the messages to pass to the OpenAI endpoint.\n",
    "\n",
    "#### Important Phrases\n",
    "- [ ] models prompts\n",
    "- [ ] prompt templates\n",
    "- [ ] structured output parser\n",
    "- [ ] output pausing\n",
    "- [ ] good prompts\n",
    "- [ ] English speaking customer service agent\n",
    "- [ ] customer review\n",
    "- [ ] different customers\n",
    "- [ ] prompt engineering\n",
    "- [ ] delivery day schema\n",
    "- [ ] customer warranty\n",
    "- [ ] English pirate\n",
    "- [ ] other languages\n",
    "- [ ] model pauses output\n",
    "- [ ] LM output JSON\n",
    "\n",
    "#### Topic Detection\n",
    "- [ ] Programming Languages\n",
    "- [ ] Artificial Intelligence\n",
    "- [ ] Web Conferencing\n",
    "- [ ] Bath And Shower\n",
    "- [ ] Language Learning\n",
    "- [ ] Web Design And HTML\n",
    "- [ ] Desktop Publishing\n",
    "- [ ] Email\n",
    "- [ ] Recalls\n",
    "- [ ] Computer Software And Applications\n",
    "- [ ] Sales And Promotions\n",
    "- [ ] Candle And Soap Making\n",
    "- [ ] Flower Shopping\n",
    "- [ ] Sales\n",
    "- [ ] Gifts And Greetings Cards\n",
    "- [ ] Party Supplies And Decorations\n",
    "- [ ] Data Storage And Warehousing\n",
    "\n",
    "\n",
    "### Transcript\n",
    "\n",
    "You in lesson one, we'll be covering models prompts and parsers. So models refers to the language models. Underpinning a lot of it, prompts refers to the style of creating inputs to pass into the models.\n",
    "\n",
    "And then parsers is on the opposite end. It involves taking the output of these models and parsing it into a more structured format so that you can do things downstream with it. So when you build an application using an LM, there'll often be reusable models.\n",
    "\n",
    "We repeatedly prompt a model pauses output and so Lanchain gives an easy set of abstractions to do this type of operation. So with that, let's jump in and take a look at models prompts and pauses. So, to get started, here's a little bit of starter code.\n",
    "\n",
    "I'm going to import OS, import OpenAI and load my OpenAI secret key. The OpenAI library is already installed in my Jupyter notebook environment. If you're running this locally and you don't have OpenAI installed yet, you might need to run that bank pip install OpenAI, but I'm not going to do that here.\n",
    "\n",
    "And then here's a helper function. This is actually very similar to the helper function that you might have seen in the Chat GPT prompt engineering for Developers course that I offered together with OpenAI's user fulfilled. And so with this helper function, you can say get completion on what is one plus one.\n",
    "\n",
    "And this will call Chat GPD or technically the model GPD 3.5 Turbo to give you an answer back like this. Now, to motivate the line chain abstractions for model prompts and pauses, let's say you get an email from a customer in a language other than English in order to make sure this is accessible.\n",
    "\n",
    "The other language I'm going to use is the English pirate language. When it comes to this R, I'd be fuming that me blended lid flew off and splattered my kitchen walls with smoothie. And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen.\n",
    "\n",
    "I need your help right now, Mati. And so what we will do is ask this om to translate the text to American English in a calm and respectful tone. So I'm going to set style to American English in a calm and respectful tone.\n",
    "\n",
    "And so in order to actually accomplish this, if you've seen a little bit of prompting before, I'm going to specify the prompt using an F string with the instructions. Translate the text that is delimited by triple bactics into style that is style, and then plug in these two styles. And so this generates a prompt that says translate the text and so on.\n",
    "\n",
    "I encourage you to pause the video and run the code and also try modifying the prompt to see if you can get a different output. You can then prompt the large language model to get a response. Let's see what the response is.\n",
    "\n",
    "Says translated the English pirate's message into this very polite. I'm really frustrated and my Blender Lit flew off and made a mess of my kitchen walls with smoothie and so on. I could really use your help right now, my friend.\n",
    "\n",
    "That sounds very nice. So if you have different customers writing reviews in different languages, not just English pirate but French, German, Japanese and so on, you can imagine having to generate a whole sequence of prompts to generate such translations. Let's look at how we can do this in a more convenient way using Lanchain.\n",
    "\n",
    "I'm going to import chat OpenAI. This is lanchain's abstraction for the chat GPT API endpoint. And so if I then set chat equals chat OpenAI and lookalike chat is it creates this object as follows that uses the Chat GPT model which is also called GPD 3.5\n",
    "\n",
    "Turbo. When I'm building applications, one thing I will often do is set the temperature parameter to be equal to zero. So the default temperature is 0.7.\n",
    "\n",
    "But let me actually redo that with temperature equals 0.0. And now the temperature is set to zero to make its output a little bit less random. And now let me define the template string as follows translate the text delimited by triple vactex into style, that is style.\n",
    "\n",
    "And then here's the text. And to repeatedly reuse this template, let's import langchain's chat prompt template. And then let me create a prompt template.\n",
    "\n",
    "Using that template string that we just wrote above. From the prompt template you can actually extract the original prompt and it realizes that this prompt has two input variables, the style and the text which were shown here with the curly braces. And here is the original template as well that we had specified.\n",
    "\n",
    "In fact, it would print this out. It realizes it has two input variables style and text. Now let's specify the style.\n",
    "\n",
    "This is a style that I want the customer message to be translated to. So I'm going to call this customer style and here's my same customer email as before. And now if I create custom messages, this will generate the prompt and we'll pass this a large language more than a minute to get a response.\n",
    "\n",
    "So if you want to look at the types, the custom message is actually a list. And if you look at the first element of the list, this is more or less that prompt that you would expect this to be creating. Lastly, let's pass this prompt to the LM.\n",
    "\n",
    "So I'm going to call Chat, which we had set earlier as a reference to the OpenAI chat GPD endpoint. And if we print out the customer responses content, then it gives you back this text translated from English pirate to polite American English. And of course you can imagine other use cases where the customer emails are in other languages.\n",
    "\n",
    "And this tool can be used to translate the messages for an English speaking to understand and reply to. I encourage you to pause the video and run the code and also try modifying the prompt to see if you can get a different output. Now let's hope our customer service Agent reply to the customer in their original language.\n",
    "\n",
    "So let's say English speaking customer service agent writes this and say hey there, customer warranty does not cover clean expenses for a kitchen because it's your fault that you misuse your blender by forgetting to put on the lid. Tough luck. SIA not a very polite message, but let's say this is what a customer service Agent wants.\n",
    "\n",
    "We are going to specify that the service message is going to be translated to this pirate style. So we want it to be in a polite tone that speaks in English pirate. And because we previously created that prompt template, the cool thing is we can now reuse that prompt template and specify that the output style we want is this service style pirate, and the text is this service reply.\n",
    "\n",
    "And if we do that, that's the prompt, and if we prompt chat GPT, this is the response it gives us backy. I must kindly inform you that the warranty be not covering the expenses or cleaning your galley and so on. Aye, tough luck.\n",
    "\n",
    "Farewell Miharti. So you might be wondering why are we using prompt templates instead of just an F string? The answer is that as you build sophisticated applications, prompts can be quite long and detailed. And so prompt templates are a useful abstraction to help you reuse good prompts when you can.\n",
    "\n",
    "This is an example of a relatively long prompt to grade a student submission for online learning application. And a prompt like this can be quite long in which you can ask the LM to first solve the problem and then have the output in a certain format and output in a certain format. And wrapping this in a Lang chain prompt makes it easier to reuse a prompt like this.\n",
    "\n",
    "Also, you see later that Lanchain provides prompts for some common operations such as summarization or question answering, or connecting to SQL databases, or connecting to different APIs. And so by using some of Langchain's built in prompts, you can quickly get an application working without needing to engineer your own prompts. One other aspect of Lanchain's prompt libraries is that it also supports output pausing, which we'll get to in a minute.\n",
    "\n",
    "But when you're building a complex application using an LLM, you often instruct the LLM to generate its output in a certain format, such as using specific keywords. This example on the left illustrates using an LLM to carry out something called chain of thought reasoning using a framework called the React framework. But don't worry about the technical details.\n",
    "\n",
    "But the keys of that is that the thought is what the LM is thinking because by giving an LM space to think it can often get to more accurate conclusions then action as a keyword to carry the specific action and then observation to show what it learned from that action and so on. And if you have a prompt that instructs the LM to use these specific keywords thought, action and observation, then this prompt can be coupled with a parser to extract out the text that has been tagged with these specific keywords. And so that together gives a very nice abstraction to specify the input to an LM, and then also have a parser correctly interpret the output that the LM gives.\n",
    "\n",
    "And so with that, let's return to see an example of an output parser using langchain. In this example, let's take a look at how you can have an LM output JSON and use langchain to parse that output. And the running example that I'll use will be to extract information from a product review and format that output in adjacent format.\n",
    "\n",
    "So here's an example of how you would like the output to be formatted. Technically, this is a Python dictionary where whether or not the product is a gift master falls. The number of days it took deliverer was five, and the price value was pretty affordable.\n",
    "\n",
    "So this is one example of a desired output. Here is an example of customer review as well as a template to try to get to that JSON output. So here's a customer review.\n",
    "\n",
    "It says, this leap blower is pretty amazing. It has four settings candleblower, gender, breeze, windy city and tornado. It arrived in two days, just in time for my wife's anniversary present.\n",
    "\n",
    "I think my wife liked it so much she was speechless. So far I've been the only one using it, and so on. And here's a review template for the following text extract the following information specify was this a GIF? So in this case it would be yes, because this is a GIF.\n",
    "\n",
    "And also delivery days. How long did it take to deliver? It looks like in this case it arrived in two days. And what's the price value? Slightly more expensive, other leaf blowers and so on.\n",
    "\n",
    "So the review template asks the om to take us input a customer review and extract these three fields and then format the output as JSON with the following keys. All right, so here's how you can wrap this in line chain. Let's import the chat prompt template.\n",
    "\n",
    "We'd actually imported this already earlier, so technically this line is redundant, but I'll just import it again and then have the prompt template created from the review template up on top. And so here's the prompt template. And now, similar to our earlier usage of a prompt template, let's create the messages to pass to the OpenAI endpoint, create the open AI endpoint, call that endpoint, and then let's print out the response.\n",
    "\n",
    "I encourage you to pause the video and run the code. And there it is. It says give us true delivery days is two, and the price value also looks pretty accurate.\n",
    "\n",
    "But note that if we check the type of the response, this is actually a string. So it looks like JSON and looks like it has key value pairs, but it's actually not a dictionary. This is just one long string.\n",
    "\n",
    "So what I really like to do is go to the response content and get the value from the gift key, which should be true, but I run this, this should generate an error because, well, this is actually a string, this is not a Python dictionary. So let's see how we would use Nychain's parser. In order to do this, I'm going to import response schema and structured output parser from Lanchain and I'm going to tell it what I wanted to pause by specifying these response schemas.\n",
    "\n",
    "So the gift schema is named gift and here's the description was the item purchased a gift for someone else? Answer true of yes, false if not to unknown and so on. So have a gift schema, delivery day schema, price value schema and then let's put all three of them into a list as follows. Now that I specify the schema for these, lanchain can actually give you the prompt itself by having the output posit tell you what instructions it wants you to send to the LM.\n",
    "\n",
    "So if I were to print format instructions, she has a pretty precise set of instructions for the LM that will cause it to generate an output that the output parser can process. So here's the new review template. And the review template includes the format instructions that langchain generated and so can create a prompt from the review template too and then create the messages that will pass to the OpenAI endpoint.\n",
    "\n",
    "If you want you can take a look at the actual prompt which gives the instructions to extract the fields gift delivery days, price value. Here's the text and then here are the formatting instructions. Finally, if we call the openi endpoint, let's take a look at what response we got.\n",
    "\n",
    "It is now this. And now if we use the output parser that we created earlier, you can then pause this into an output dictionary by print. Looks like this.\n",
    "\n",
    "And notice that this is of type dictionary, not a string. Which is why I can now extract the value associated with the key gift and get true or the value associated with delivery days and get two. Or you can also extract the value associated with price value.\n",
    "\n",
    "So this is a nifty way to take your LM output and parse it into a Python dictionary. To make the output easier to use in downstream processing, I encourage you to pause the video and run the code and so that's it for models, prompt and parsers. With these tools, hopefully you'll be able to reuse your own prompt templates easily share prompt templates with others that you're collaborating with, even use linechain's built in prompt templates, which, as you just saw, can often be coupled with.\n",
    "\n",
    "An output parser so that the input prompt to output in a specific format and then a parser parses that output to store the data in a Python dictionary or some other data structure that makes it easy for downstream processing. I hope you find this useful in many of your applications. And with that, let's go on to the next video where we'll see how Zanking can help you build better chat bots or have an LM have more effective chats by better managing what it remembers from the conversation you've had so far.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:25:54.604890Z",
     "start_time": "2023-06-08T14:25:54.601402Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import credentials\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = credentials. OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chat API : OpenAI\n",
    "Let's start with a direct API calls to OpenAI."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:25:55.806456Z",
     "start_time": "2023-06-08T14:25:55.804161Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'As an AI language model, I can tell you that the answer to 1+1 is 2.'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:25:59.054239Z",
     "start_time": "2023-06-08T14:25:56.498916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:26:17.161836Z",
     "start_time": "2023-06-08T14:26:17.157268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:27:26.707646Z",
     "start_time": "2023-06-08T14:27:26.696882Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:27:34.734511Z",
     "start_time": "2023-06-08T14:27:34.722817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite upset that my blender lid came off and caused my smoothie to splatter all over my kitchen walls. Additionally, the warranty does not cover the cost of cleaning up the mess. Would you be able to assist me, please? Thank you kindly.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:27:53.781369Z",
     "start_time": "2023-06-08T14:27:48.289235Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat API : LangChain\n",
    "Let's try how we can do the same using LangChain."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:31:07.325160Z",
     "start_time": "2023-06-08T14:31:07.322572Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:31:18.865624Z",
     "start_time": "2023-06-08T14:31:18.862480Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "chat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:31:19.228546Z",
     "start_time": "2023-06-08T14:31:19.216121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:31:32.676432Z",
     "start_time": "2023-06-08T14:31:32.663843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:31:42.653839Z",
     "start_time": "2023-06-08T14:31:42.641067Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:31:49.146921Z",
     "start_time": "2023-06-08T14:31:49.134398Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['style', 'text']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:00.229904Z",
     "start_time": "2023-06-08T14:32:00.218331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:07.112882Z",
     "start_time": "2023-06-08T14:32:07.101826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:14.490134Z",
     "start_time": "2023-06-08T14:32:14.476880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:21.006313Z",
     "start_time": "2023-06-08T14:32:20.991959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:26.856419Z",
     "start_time": "2023-06-08T14:32:26.844411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:33.361990Z",
     "start_time": "2023-06-08T14:32:33.351285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:48.790889Z",
     "start_time": "2023-06-08T14:32:43.610249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:49.689915Z",
     "start_time": "2023-06-08T14:32:49.666358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:32:56.931427Z",
     "start_time": "2023-06-08T14:32:56.916903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:33:06.046771Z",
     "start_time": "2023-06-08T14:33:06.032135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:33:13.291401Z",
     "start_time": "2023-06-08T14:33:13.276844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! I must kindly inform ye that the warranty be not coverin' the expenses o' cleaning yer galley, as 'tis yer own fault fer misusin' yer blender by forgettin' to put the lid on afore startin' it. Aye, tough luck! Farewell and may the winds be in yer favor!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:33:27.618036Z",
     "start_time": "2023-06-08T14:33:19.848207Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:34:12.620350Z",
     "start_time": "2023-06-08T14:34:12.607618Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:34:21.399479Z",
     "start_time": "2023-06-08T14:34:21.384267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:34:29.231105Z",
     "start_time": "2023-06-08T14:34:29.218619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1ac59e8c361f6508ed6bca511ca80a0c in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "print(response.content)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:35:13.872552Z",
     "start_time": "2023-06-08T14:34:37.818242Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:35:13.879675Z",
     "start_time": "2023-06-08T14:35:13.876057Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# You will get an error by running this line of code \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# because'gift' is not a dictionary\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# 'gift' is a string\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgift\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code\n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:35:13.923518Z",
     "start_time": "2023-06-08T14:35:13.879961Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:35:50.260193Z",
     "start_time": "2023-06-08T14:35:50.254703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema,\n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:35:50.598188Z",
     "start_time": "2023-06-08T14:35:50.594576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:35:57.683250Z",
     "start_time": "2023-06-08T14:35:57.671196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:03.132136Z",
     "start_time": "2023-06-08T14:36:03.119054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:09.502079Z",
     "start_time": "2023-06-08T14:36:09.489331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review,\n",
    "                                format_instructions=format_instructions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:16.487949Z",
     "start_time": "2023-06-08T14:36:16.468179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:28.177482Z",
     "start_time": "2023-06-08T14:36:28.162880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:39.529705Z",
     "start_time": "2023-06-08T14:36:34.653127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:42.614154Z",
     "start_time": "2023-06-08T14:36:42.595053Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:52.202021Z",
     "start_time": "2023-06-08T14:36:52.189296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "{'gift': True,\n 'delivery_days': '2',\n 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:36:57.809728Z",
     "start_time": "2023-06-08T14:36:57.780491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:37:03.762096Z",
     "start_time": "2023-06-08T14:37:03.748909Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "'2'"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:37:09.684732Z",
     "start_time": "2023-06-08T14:37:09.670389Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
